{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd78588",
   "metadata": {},
   "source": [
    "# ------------- PROYECTO FINAL G & S--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e16c",
   "metadata": {},
   "source": [
    "## LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 #librería para manejo de imágenes y video\n",
    "import numpy as np #librería para manejo de arreglos y matrices\n",
    "import mediapipe as mp #librería para detección de gestos y poses\n",
    "import pickle #librería para serializar objetos de Python\n",
    "import os #librería para manejar archivos y directorios\n",
    "import time #librería para manejar el tiempo\n",
    "\n",
    "# Librerías para manejo de datos\n",
    "import tensorflow as tf #librería para manejo de redes neuronales y aprendizaje profundo\n",
    "from tensorflow.keras.models import Sequential, load_model #librería para manejo de modelos de Keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization #librería para manejo de capas de Keras\n",
    "from tensorflow.keras.optimizers import Adam #librería para manejo de optimizadores de Keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau #librería para manejo de callbacks de Keras \n",
    "from tensorflow.keras.regularizers import l2 #librería para manejo de regularizadores de Keras\n",
    "\n",
    "# Librerías para manejo de datos tabulares\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder #librería para manejo de escaladores y codificadores de etiquetas\n",
    "from sklearn.model_selection import train_test_split #librería para manejo de división de datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report #librería para manejo de métricas de clasificación\n",
    "\n",
    "# Comunicación y cámara\n",
    "import socket #librería para manejo de sockets y comunicación en red\n",
    "import queue #librería para manejo de colas\n",
    "\n",
    "# Voz\n",
    "import pyttsx3 # librería para síntesis de voz\n",
    "import threading # librería para manejo de hilos\n",
    "\n",
    "# Módulos para reconocimiento de voz\n",
    "import speech_recognition as sr # librería para reconocimiento de voz\n",
    "import librosa # librería para análisis de audio\n",
    "import io # librería para manejo de flujos de entrada/salida\n",
    "import wave # librería para manejo de archivos de audio en formato WAV\n",
    "\n",
    "# Generador de audios\n",
    "from pydub import AudioSegment # librería para manejo de archivos de audio\n",
    "from pydub.silence import detect_nonsilent # librería para detección de silencios en archivos de audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd329ca",
   "metadata": {},
   "source": [
    "## UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para el reconocimiento de voz\n",
    "SAMPLE_RATE_IN = 48000  # Tasa de muestreo del micrófono INMP441 en Hz\n",
    "SAMPLE_RATE_OUT = 16000  # Tasa de muestreo requerida por la API de reconocimiento de voz (Google, etc.)\n",
    "BUFFER_DURATION = 5  # Duración en segundos del buffer de audio que se acumula antes de enviar (5 s)\n",
    "\n",
    "\n",
    "UDP_IP_PI = \"192.168.7.2\"  # IP de la Raspberry Pi donde corre el receptor UDP\n",
    "UDP_OPEN = '0.0.0.0' # IP local de enlace para recibir en cualquier interfaz\n",
    "\n",
    "# Puertos para diferentes servicios\n",
    "\n",
    "UDP_PORT_SERVO = 5001      # envío de comandos de ángulo al servo\n",
    "UDP_PORT_PARLANTE = 5003   # recepción de audio TTS para reproducir\n",
    "UDP_PORT_CAM = 5002        # envío/recepción de fragmentos de vídeo\n",
    "UDP_PORT_MICROFONO = 5006  # envío de audio capturado por micrófono\n",
    "UDP_PORT_TEXT = 5005       # recepción de texto transcrito (API → PC)\n",
    "\n",
    "# Agregar al inicio del archivo\n",
    "UDP_PORT_HANDS_STATUS = 5007  # Nuevo puerto\n",
    "sock_hands_status = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# ---- Agregar al inicio del archivo ----\n",
    "UDP_PORT_HEARTBEAT = 5008\n",
    "sock_heartbeat = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# MSS\n",
    "MAX_PACKET_SIZE = 1460  # Tamaño máximo en bytes de cada paquete UDP (MTU 1500 – cabeceras)\n",
    "\n",
    "# Buffer UDP\n",
    "BUFFER_UDP = 65536 # Tamaño máximo del buffer de recepción UDP en bytes (~64 KiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23775bec",
   "metadata": {},
   "source": [
    "1.\tSAMPLE_RATE_IN = 48000\n",
    "Define a qué frecuencia (en muestras por segundo) el micrófono INMP441 está capturando audio. Un valor alto (48 kHz) mejora la calidad de captura.\n",
    "2.\tSAMPLE_RATE_OUT = 16000\n",
    "Muchas APIs de reconocimiento de voz (por ejemplo Google Speech-to-Text) requieren audio a 16 kHz. Este parámetro se usa para re-muestrear el buffer antes de enviarlo.\n",
    "3.\tBUFFER_DURATION = 5\n",
    "Indica que el sistema acumulará 5 segundos de audio en un buffer antes de re-muestrear y enviar todo el bloque a la API. Controla la latencia de transcripción.\n",
    "4.\tUDP_IP_PI = \"192.168.7.2\"\n",
    "Dirección IP fija de la Raspberry Pi embebida. El script de recolección envía ahí el audio y otros comandos.\n",
    "5.\tUDP_OPEN = '0.0.0.0'\n",
    "Al enlazar un socket receptor, se usa esta IP para que escuche en todas las interfaces de red disponibles del host.\n",
    "6.\tUDP_PORT_MICROFONO = 5006\n",
    "Puerto UDP donde el script envía bloques de audio PCM originados en el micrófono.\n",
    "7.\tUDP_PORT_TEXT = 5005\n",
    "Puerto UDP donde el script recibe las cadenas de texto transcrito que debe mostrar en pantalla.\n",
    "8.\tUDP_PORT_SERVO = 5001\n",
    "Puerto UDP utilizado para enviar desde el PC los ángulos a los que debe moverse el servomotor de las gafas.\n",
    "9.\tUDP_PORT_PARLANTE = 5003\n",
    "Puerto UDP donde la Raspberry Pi recibe el audio TTS (voz sintetizada) enviado por el PC tras reconocimiento de gesto.\n",
    "10.\tUDP_PORT_CAM = 5002\n",
    "Puerto UDP donde se fragmentan y envían los frames de vídeo codificados por la Pi hacia el PC.\n",
    "11.\tUDP_PORT_HANDS_STATUS = 5007\n",
    "Define un nuevo puerto UDP (5007) dedicado a recibir desde la PC el estado de detección de manos (“HANDS_DETECTED” o “NO_HANDS”). Esto permite que la Raspberry Pi sepa si debe activar o desactivar el servo o la lógica de visualización en función de si hay una mano visible.\n",
    "12.\tsock_hands_status = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "Crea un socket UDP (AF_INET y SOCK_DGRAM) que se enlazará al puerto anterior para recibir mensajes de estado de manos. No se bloquea, para poder integrar su lectura en el bucle principal o en un select.\n",
    "13.\tUDP_PORT_HEARTBEAT = 5008\n",
    "Establece otro puerto UDP (5008) para recibir señales de latido (“heartbeat”) periódicas desde la PC. Estas señales garantizan que la conexión sigue viva; si deja de llegar el heartbeat, la Pi asume que la PC o la conexión falló y puede desactivar componentes por seguridad.\n",
    "14.\tsock_heartbeat = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "Crea un socket UDP para recibir los mensajes de heartbeat. Al igual que con sock_hands_status, se usa AF_INET y SOCK_DGRAM y se integrará en el mismo mecanismo de lectura no bloqueante (por ejemplo, usando select) junto con los otros sockets.\n",
    "15.\tMAX_PACKET_SIZE = 1460\n",
    "Ajusta el tamaño máximo útil de datos por paquete UDP para no fragmentar a nivel IP (1500 MTU – 20 bytes IP – 20 bytes UDP).\n",
    "16.\tBUFFER_UDP = 65536\n",
    "Reserva 64 KiB para el buffer de sockets UDP, permitiendo recibir paquetes grandes o ráfagas antes de desbordar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51933c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para enviar heartbeat\n",
    "def send_heartbeat(): #Declara la función send_heartbeat que se encargará de enviar pulsos (“heartbeats”) periódicos a la Raspberry Pi.\n",
    "    while True: #Inicia un bucle infinito, de modo que se envíe un heartbeat cada intervalo definido.\n",
    "        try:\n",
    "            sock_heartbeat.sendto(b\"HEARTBEAT\", (UDP_IP_PI, UDP_PORT_HEARTBEAT)) #Usa el socket UDP sock_heartbeat para enviar el mensaje de bytes b\"HEARTBEAT\" a la dirección (UDP_IP_PI, UDP_PORT_HEARTBEAT) (la IP y el puerto de la Pi). Esto informa al receptor de que el enlace todavía está activo.\n",
    "            time.sleep(1) #función durante 1 segundo, controlando la frecuencia de envío (1 Hz).\n",
    "        except Exception as e:\n",
    "            break  # Salir si hay error\n",
    "\n",
    "# Iniciar hilo de heartbeat al comenzar el programa\n",
    "heartbeat_thread = threading.Thread(target=send_heartbeat, daemon=True) #Crea un hilo de ejecución (Thread) que ejecutará la función send_heartbeat. El parámetro daemon=True indica que este hilo no bloqueará la finalización del programa; se cerrará automáticamente cuando el programa principal termine.\n",
    "heartbeat_thread.start() #Arranca el hilo, que inmediatamente comenzará a ejecutar send_heartbeat() en paralelo al resto del script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99382564",
   "metadata": {},
   "source": [
    "Con esto, el sistema garantiza que, mientras el programa principal esté corriendo, se enviará un heartbeat cada segundo a la Raspberry Pi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838398c2",
   "metadata": {},
   "source": [
    "## CAMARA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb60538",
   "metadata": {},
   "source": [
    "### MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands #Accede al módulo de MediaPipe especializado en manos. A partir de aquí puedes invocar clases y funciones para detección y seguimiento de landmarks de la mano\n",
    "hands = mp_hands.Hands( #Crea una instancia del detector de manos con los siguientes parámetros:\n",
    "    static_image_mode=False, #Indica que el modelo espera vídeo continuo. Si fuese True, trataría cada frame como imagen independiente (más lento).\n",
    "    max_num_hands=2, #Número máximo de manos que intentará detectar en cada frame. Aquí se deja en 2 para cubrir casos de gestos con ambas manos.\n",
    "    min_detection_confidence=0.5, #Umbral mínimo (0.0–1.0) de confianza para reportar una detección de mano. Con 0.5 se filtran detecciones débiles; bajar a 0.4 aumentaría sensibilidad (más falsos positivos).\n",
    "    min_tracking_confidence=0.5 #Umbral mínimo para continuar rastreando una mano ya detectada entre frames. Un valor bajo (p. ej. 0.4) permite que el tracker mantenga seguimiento aún con oclusiones parciales.\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils #Obtiene el módulo de utilidades gráficas de MediaPipe. Con él puedes dibujar en el frame los landmarks y conexiones de la mano (esqueleto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14f25",
   "metadata": {},
   "source": [
    "### COMUNICACION CAMARA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43da7e",
   "metadata": {},
   "source": [
    "Usada para recibir imágenes en tiempo real enviadas por la Raspberry Pi mediante paquetes UDP fragmentados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d6d35",
   "metadata": {},
   "source": [
    "Permite recibir fragmentos de imagen JPEG enviados por UDP desde la Raspberry Pi, reconstruirlos, y entregarlos como un frame OpenCV utilizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = UDP_OPEN #Escucha en todas las interfaces de red (IP 0.0.0.0).\n",
    "        self.port = UDP_PORT_CAM #Puerto donde se reciben los fragmentos de imagen enviados por la cámara (Raspberry Pi).\n",
    "        self.buffer_size = BUFFER_UDP #Define el tamaño del buffer de recepción de paquetes. Típicamente 65536 bytes.\n",
    "        self.mtu = MAX_PACKET_SIZE #Máximo tamaño de fragmento que puede recibir sin que se fragmente a nivel IP (normalmente 1460 bytes).\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #Crea el socket UDP que se usará para escuchar los fragmentos de imagen.\n",
    "        self.sock.settimeout(2) #Establece un tiempo máximo de espera (2 segundos) para que recvfrom no bloquee indefinidamente.\n",
    "        self.frame = None #Contendrá el último frame completo decodificado en formato OpenCV.\n",
    "        self.fragments = [] #Lista temporal que almacena los fragmentos UDP de una imagen hasta que estén todos.\n",
    "        self.running = False #Bandera booleana que controla si el hilo de recepción está activo.\n",
    "        self.thread = None #Referencia al hilo que recibe paquetes en segundo plano.\n",
    "        self.lock = threading.Lock() #Lock para sincronizar acceso entre hilos a self.frame y self.fragments.\n",
    "        self.start() #Llama al método start() automáticamente al construir la clase.\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running: #Evita que se inicie el hilo más de una vez.\n",
    "            self.running = True #Asocia el socket al host y puerto definidos para que escuche los paquetes entrantes.\n",
    "            self.sock.bind((self.host, self.port)) #Crea un hilo nuevo que ejecutará la función _receive_frames.\n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True) #Asegura que el hilo se cierra automáticamente al terminar el programa principal.\n",
    "            self.thread.start() #Inicia el hilo que estará escuchando los fragmentos de imagen.\n",
    "\n",
    "    def _receive_frames(self): # Función que se ejecuta en un hilo separado para recibir los fragmentos de imagen.\n",
    "        while self.running: # Bucle que se ejecuta mientras self.running sea True.\n",
    "            try:\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size) # Recibe un fragmento de imagen (máximo 1460 bytes).\n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment) #Almacena el fragmento en la lista temporal.\n",
    "                    if len(fragment) < self.mtu:  # Si el fragmento recibido es más pequeño que el máximo permitido, se asume que es el último fragmento de la imagen.\n",
    "                        frame_bytes = b''.join(self.fragments) #Une todos los fragmentos acumulados en un único bloque binario.\n",
    "                        self.fragments = [] # Limpia la lista de fragmentos para recibir una nueva imagen.\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8) #Convierte el bloque binario a un array NumPy de bytes.\n",
    "                        self.frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR) #Decodifica la imagen JPEG para obtener un frame válido de OpenCV.\n",
    "\n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error en recepción: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()\n",
    "            return False, None\n",
    "    \"\"\" \tSi hay uno válido, devuelve (True, frame)\n",
    "            Si no, devuelve (False, None)\n",
    "            copy() asegura que no se modifique accidentalmente el frame mientras otro hilo lo lee.\"\"\"\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "\n",
    "        \"\"\" Marca running como falso → el hilo terminará.\n",
    "            Limpia frame y fragments.\n",
    "            Espera que el hilo se cierre.\n",
    "            Cierra el socket UDP. \"\"\"\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release() #Destructor de la clase. Asegura que al eliminar la instancia, todos los recursos se limpien correctamente (buen diseño)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cd388",
   "metadata": {},
   "source": [
    "### MODELO TFLITE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e17ad0",
   "metadata": {},
   "source": [
    "Encargada de cargar y ejecutar el modelo optimizado en TensorFlow Lite:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define una clase para encapsular la carga y ejecución de un modelo TensorFlow Lite.\n",
    "class TFLiteModel:\n",
    "    def __init__(self, model_path): #Constructor que recibe la ruta al archivo .tflite del modelo optimizado\n",
    "        \n",
    "        # Cargar el modelo TFLite\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path) #Crea un intérprete de TensorFlow Lite cargando los pesos y la arquitectura desde el fichero model_path.\n",
    "        self.interpreter.allocate_tensors() #Reserva memoria para todos los tensores de entrada y salida del modelo en el intérprete. Debe llamarse antes de cualquier inferencia.\n",
    "        \n",
    "        # Obtener detalles de entrada y salida\n",
    "        self.input_details = self.interpreter.get_input_details() #Recupera metadatos sobre las entradas del modelo: nombre, índice, forma esperada (shape) y tipo de dato (dtype).\n",
    "        self.output_details = self.interpreter.get_output_details() #De manera análoga, recupera metadatos sobre las salidas del modelo.\n",
    "    \n",
    "    def predict(self, input_data): #Método público para hacer una inferencia dado un vector de características input_data.\n",
    "        # Asegurar el tipo de dato correcto y agregar dimensión batch si es necesario\n",
    "        input_data = np.array(input_data, dtype=self.input_details[0]['dtype']) #Convierte la entrada a un array NumPy con el tipo de dato que el modelo espera (por ejemplo float32).\n",
    "        #Comprobar y ajustar batch dimension\n",
    "        if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \"\"\"o\tSi input_data viene como (n_features,) (una sola muestra), esta condición detecta que \n",
    "        falta la dimensión de batch y la añade con expand_dims, convirtiéndolo en (1, n_features).\"\"\"\n",
    "\n",
    "        # Establecer la entrada y ejecutar la inferencia\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data) #Copia los datos de input_data al tensor de entrada interno del intérprete, usando el índice que corresponde a la primera (y única) entrada.\n",
    "        self.interpreter.invoke() #Ejecuta la inferencia: el intérprete procesa el grafo TFLite usando los tensores de entrada ya cargados, y produce valores en los tensores de salida.\n",
    "        \n",
    "        # Obtener la salida\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index']) #Recupera los datos del tensor de salida (por ejemplo, el vector de probabilidades resultante de la capa softmax), usando el índice de salida.\n",
    "        return output_data #Devuelve el array NumPy con la salida de la inferencia, que luego se procesa (por ejemplo, con np.argmax) para obtener la clase predicha.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338e79a",
   "metadata": {},
   "source": [
    "Con esta clase, el código principal puede crear instancias así:\n",
    "\n",
    "model = TFLiteModel('modelo_optimizadotl_v15.tflite')\n",
    "\n",
    "probs = model.predict(feature_vector)\n",
    "\n",
    "predicted_class = np.argmax(probs)\n",
    "\n",
    "y de esta forma aislar toda la complejidad de TensorFlow Lite en un envoltorio muy sencillo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253769a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU') #Consulta a TensorFlow si hay dispositivos físicos del tipo GPU disponibles en el sistema. Devuelve una lista de objetos Device.\n",
    "if physical_devices: #Si la lista no está vacía (existen GPUs), entra al bloque.\n",
    "    # Configuración de TensorFlow para rendimiento en CPU\n",
    "    try:\n",
    "        # Verificar si hay GPU disponible (para futuras expansiones)\n",
    "        physical_devices = tf.config.list_physical_devices('GPU') #Vuelve a obtener la lista de GPUs (repetición intencional o por comodidad de contexto).\n",
    "        \n",
    "        if physical_devices: #Confirma nuevamente la presencia de GPUs.\n",
    "            # Configuración para GPU (no se ejecutará en tu caso)\n",
    "            for device in physical_devices: #Itera sobre cada GPU detectada.\n",
    "                tf.config.experimental.set_memory_growth(device, True) #Activa el crecimiento dinámico de memoria en GPU: TensorFlow reservará solo lo necesario en lugar de toda la memoria GPU al inicio.\n",
    "            print(\"GPU disponible para aceleración\")\n",
    "        else:\n",
    "            # Optimización para CPU\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(4)  # Aprovecha núcleos físicos\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(2)  # Paralelismo entre operaciones\n",
    "            print(\"Modo CPU activado: Configuración optimizada para Intel Core i7-7500U\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error de configuración: {str(e)}\")\n",
    "        print(\"Usando configuración por defecto de CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825cbf8",
   "metadata": {},
   "source": [
    "Con esto, el script ajusta TensorFlow para maximizar rendimiento en función del hardware disponible, sin bloquear la ejecución si algo falla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f84a7",
   "metadata": {},
   "source": [
    "### ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb605c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_v15\" #Define la ruta (nombre de carpeta) donde se almacenarán todos los archivos relacionados con la recolección de datos y el entrenamiento.\n",
    "os.makedirs(data_dir, exist_ok=True) #Crea la carpeta hand_gestures_data_v15 si no existe. exist_ok=True evita lanzar excepción si la carpeta ya está creada.\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None #Inicializa la variable que contendrá el modelo Keras. Se asignará más adelante una instancia de Sequential tras definir la arquitectura.\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler() #Crea un objeto StandardScaler vacío, que luego se ajustará (fit) sobre los datos de entrenamiento para calcular medias y desviaciones.\n",
    "label_encoder = LabelEncoder() #Crea un objeto LabelEncoder que traducirá las etiquetas de texto (nombres de seña) a índices numéricos.\n",
    "model_file = \"hand_gesture_nn_model_v15.h5\"  #Nombre de archivo donde se guardará el modelo Keras entrenado en formato HDF5.\n",
    "scaler_file = \"hand_gesture_scaler_v15.pkl\" #Ruta donde se serializará el objeto StandardScaler con pickle.\n",
    "encoder_file = \"hand_gesture_encoder_v15.pkl\" #Archivo para el objeto LabelEncoder serializado.\n",
    "gesture_data = \"gesture_data_v15.pkl\"  #Nombre del archivo .pkl que contendrá el dataset completo de vectores de características y etiquetas reunidos.\n",
    "model_tflite = \"modelo_optimizadotl_v15.tflite\" #Ruta del archivo de salida para el modelo convertido a TensorFlow Lite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c29b9",
   "metadata": {},
   "source": [
    "De este modo, el script centraliza en variables todos los nombres de carpetas y archivos, facilitando su mantenimiento y evitando literales repartidos por el código.\n",
    "Aquí tienes la explicación línea por línea y por bloques del fragmento que define variables globales en el script me_15_v6_recoleccion.ipynb. Estas variables son esenciales para controlar el estado de la recolección, el entrenamiento y la evaluación del modelo de reconocimiento de gestos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ec0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = [] #Lista vacía donde se almacenarán los vectores de características (arrays NumPy) extraídos de los landmarks de las manos (coordenadas, distancias, ángulos).\n",
    "labels = [] #Lista paralela a data que guarda las etiquetas textuales asociadas a cada vector (por ejemplo \"HOLA\", \"GRACIAS\").\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False #Bandera booleana que indica si el modelo ya fue entrenado. Se usa para habilitar su uso en tiempo real o en pruebas.\n",
    "is_collecting = False #Flag que determina si actualmente se están recogiendo muestras. Se activa cuando el usuario presiona una tecla para empezar a grabar un gesto específico.\n",
    "current_gesture = \"\" #Cadena que almacena el nombre de la seña actualmente en recolección. Ejemplo: \"COMO\", \"AYUDA\".\n",
    "samples_collected = 0 #Contador de cuántas muestras se han recolectado para la seña actual. Se reinicia al iniciar la recolección de una nueva seña.\n",
    "max_samples = 5000 #Número máximo de muestras que se permite recolectar en total o por clase, dependiendo de cómo esté implementada la lógica de corte.\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0 #Guarda la última vez (en segundos desde epoch) que se agregó una muestra. Se compara contra time.time() para espaciar capturas.\n",
    "sample_delay = 0.05  # Tiempo mínimo entre muestras: 0.05 s = 50 ms.\n",
    "#Evita capturar demasiadas muestras en poco tiempo (por ejemplo si el gesto no cambia), y mejora la variedad dentro de una clase.\n",
    "\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\" #Texto que se muestra en pantalla como retroalimentación al usuario (ej: “Recolectando ‘HOLA’...”, “Modelo entrenado con éxito”, etc.).\n",
    "message_until = 0 #Marca de tiempo (en segundos) hasta la cual se mostrará el mensaje. Pasado este tiempo, el mensaje desaparece automáticamente del display.\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}\n",
    "\"\"\"Diccionario que guarda los resultados de entrenamiento del modelo:\n",
    "o\t'accuracy': precisión final en el conjunto de entrenamiento\n",
    "o\t'val_accuracy': precisión sobre el conjunto de validación\n",
    "o\t'training_time': duración total del proceso de entrenamiento (en segundos)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ce453",
   "metadata": {},
   "source": [
    "### EXTRACCION DE LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hands_detected_time = 0 #Guarda el timestamp (segundos desde epoch) de la última vez que se detectó al menos una mano, para gestionar estados “sin manos”.\n",
    "hands_detected_status = False #Flag que indica si actualmente hay manos siendo detectadas (True) o no (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # Crea un socket UDP para enviar datos (en este caso, la coordenada normalizada de la muñeca derecha al servo en la Pi).\n",
    "\n",
    "def extract_hand_landmarks(frame, send_sock=None): #Declara la función que recibe un frame (imagen BGR) y opcionalmente un socket send_sock para enviar coordenadas.\n",
    "    global last_hands_detected_time, hands_detected_status\n",
    "    \"\"\"\n",
    "    Extrae landmarks de las manos y realiza seguimiento de la muñeca derecha.\n",
    "    \n",
    "    Args:\n",
    "        frame: Imagen en formato BGR\n",
    "        send_sock: Socket UDP opcional para enviar datos de seguimiento\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (landmarks_data, hands_detected)\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #Convierte el frame de BGR (OpenCV) a RGB (formato requerido por MediaPipe).\n",
    "    results = hands.process(frame_rgb) #Pasa el frame a MediaPipe Hands para obtener results.multi_hand_landmarks y results.multi_handedness.\n",
    "    \n",
    "    landmarks_data = [] #Lista temporal para acumular valores [x,y,z] de cada landmark de ambas manos (hasta 2×21×3 = 126 valores).\n",
    "    hands_detected = False #Flag local que se pondrá a True si se detecta al menos una mano.\n",
    "    x_normalized = None #Variable para almacenar la coordenada normalizada de la muñeca derecha en rango [−7.5,7.5][-7.5,7.5].\n",
    "    right_wrist_pixel = None #Coordenadas en píxeles de la muñeca derecha para dibujar un círculo.\n",
    "    \n",
    "    if results.multi_hand_landmarks: #Comprueba si MediaPipe detectó alguna mano en el frame.\n",
    "        hands_detected = True #Marca que hay manos para este frame.\n",
    "        # Procesar ambas manos para landmarks\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks): #Itera sobre cada mano detectada (hasta max_num_hands).\n",
    "            # Dibujar landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS) #Dibuja en el frame los puntos y conexiones de la mano para visual feedback.\n",
    "            \n",
    "            # Extraer coordenadas\n",
    "            landmarks = [] #Lista temporal para esta mano.\n",
    "            for landmark in hand_landmarks.landmark: #Recorre los 21 landmarks de la mano.\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])  #Añade las tres coordenadas normalizadas de cada landmark.\n",
    "            landmarks_data.extend(landmarks) #Concatena los valores de esta mano al buffer global landmarks_data.\n",
    "            \n",
    "            # Detectar mano derecha para seguimiento\n",
    "            if results.multi_handedness: #Comprueba si se proporcionó información de lateralidad (izquierda/derecha).\n",
    "                handedness = results.multi_handedness[hand_idx] #Obtiene el resultado de clasificación para esta mano.\n",
    "                if handedness.classification[0].label == 'Left' and not x_normalized: #Aquí ‘Left’ corresponde a la mano DERECHA del usuario (configuración invertida). Se asegura de procesar solo la primera detección.\n",
    "                    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST] #Toma el landmark de la muñeca.\n",
    "                    \n",
    "                    # Calcular coordenadas normalizadas\n",
    "                    x_normalized = int((wrist.x - 0.5) * 15)  # Rango -7.5 a 7.5,Normaliza la coordenada X de [0,1][0,1] a [−7.5,7.5][-7.5,7.5]. Este valor controla el ángulo del servomotor.\n",
    "                    #print(x_normalized)\n",
    "                    \n",
    "                    # Obtener coordenadas para dibujo\n",
    "                    right_wrist_pixel = mp_drawing._normalized_to_pixel_coordinates( #Convierte la posición normalizada (wrist.x,wrist.y)(wrist.x, wrist.y) al sistema de píxeles del frame (width, height).\n",
    "                        wrist.x, wrist.y, frame.shape[1], frame.shape[0]\n",
    "                    )\n",
    "\n",
    "        # Enviar datos y dibujar si se detectó mano derecha\n",
    "        if x_normalized is not None: #Solo si se detectó muñeca derecha, envía y dibuja:\n",
    "            if send_sock:  # Solo enviar si se provee socket, Verifica que el socket se haya pasado como argumento.\n",
    "                send_sock.sendto(  #Envía la coordenada normalizada como bytes al puerto de servo de la Pi.\n",
    "                    str(x_normalized).encode(),\n",
    "                    (UDP_IP_PI, UDP_PORT_SERVO)  # Asegurar que estas constantes están definidas\n",
    "                )\n",
    "            if right_wrist_pixel:\n",
    "                cv2.circle(frame, right_wrist_pixel, 10, (0, 255, 0), -1) #Dibuja un círculo verde de radio 10 píxeles en la posición de la muñeca.\n",
    "    \n",
    "    # Rellenar con ceros si no hay manos\n",
    "    while len(landmarks_data) < 21 * 3 * 2:\n",
    "        landmarks_data.append(0.0)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "    \"\"\" o\tCada frame debe devolver siempre 126 valores (2 manos × 21 landmarks × 3 coordenadas).\n",
    "        o\tSi no se detectaron manos o faltan valores, se añaden ceros hasta completar.\n",
    "        o\tLuego se recorta con [:126] para asegurar tamaño exacto.\n",
    "    \"\"\"\n",
    "\n",
    "        # Enviar estado de detección\n",
    "    current_time = time.time()\n",
    "    if hands_detected:\n",
    "        sock_hands_status.sendto(b\"HANDS_DETECTED\", (UDP_IP_PI, UDP_PORT_HANDS_STATUS)) #notifica inmediatamente al puerto de estado.\n",
    "        last_hands_detected_time = current_time #actualiza el último instante detectado.\n",
    "        hands_detected_status = True #marca estado global.\n",
    "    else:\n",
    "        \"\"\"o\tComprueba si ha pasado más de 1 s desde la última detección.\n",
    "           o\tEntonces envía b\"NO_HANDS\" y pone hands_detected_status = False.\n",
    "        \"\"\"\n",
    "        if current_time - last_hands_detected_time > 1:  # Esperar 2 segundos sin manos\n",
    "            sock_hands_status.sendto(b\"NO_HANDS\", (UDP_IP_PI, UDP_PORT_HANDS_STATUS))\n",
    "            hands_detected_status = False\n",
    "    \n",
    "    return landmarks_data, hands_detected #Devuelve el vector fijado en longitud y un flag booleano indicando si se detectaron manos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62350ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2fc5e",
   "metadata": {},
   "source": [
    "### RECOLECCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_collection(gesture_name): #Se define la función start_collection que recibe como parámetro gesture_name, el nombre de la seña que el usuario va a recoger.\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = True #Activa el modo de recolección. A partir de este momento, el bucle principal del script empezará a capturar y almacenar muestras de gestos.\n",
    "    current_gesture = gesture_name #Guarda en la variable global el nombre de la seña que se va a recolectar. Esto se usa luego para etiquetar cada vector de características con la etiqueta correcta.\n",
    "    samples_collected = 0 #Reinicia el contador de muestras recolectadas a cero, para comenzar a contar desde el primer frame etiquetado en esta sesión de recolección.\n",
    "    set_message(f\"Mantenga la seña frente a la cámara. Recolectando '{gesture_name}'...\", 3)\n",
    "\n",
    "def stop_collection():#Define la función stop_collection sin parámetros, que detiene la recolección en curso. \n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = False #Desactiva el modo de recolección. El bucle principal dejará de capturar y almacenar muestras.\n",
    "    current_gesture = \"\" #Limpia el nombre de la seña actual, ya que ya no se está recolectando ninguna.\n",
    "    samples_collected = 0 #Reinicia el contador de muestras (opcional, para evitar residuos en próximas colecciones).\n",
    "    set_message(\"Recolección finalizada\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700669a",
   "metadata": {},
   "source": [
    "Estas dos funciones se llaman desde el bucle principal al detectar la pulsación de teclas específicas (por ejemplo, ord('a') para empezar la seña \"A\") y al pulsar la tecla de parada (q o similar). Controlan todo el flujo de recolección de datos y garantizan que cada muestra se etiquete correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7d27a",
   "metadata": {},
   "source": [
    "### GUARDADO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60408d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(): #Declara la función save_data sin parámetros, destinada a almacenar en disco las muestras y etiquetas recogidas hasta el momento.\n",
    "    global data, labels\n",
    "    data_to_save = {\"features\": data, \"labels\": labels}\n",
    "    \"\"\"Crea un diccionario que agrupa las dos listas en un único objeto:\n",
    "        o\t\"features\": contendrá todos los vectores de características (data)\n",
    "        o\t\"labels\": las etiquetas correspondientes (labels)\n",
    "    Esto facilita la carga posterior, manteniendo juntos datos y etiquetas.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(f\"{data_dir}/{gesture_data}\", \"wb\") as f:\n",
    "        \"\"\"Abre (o crea) el archivo de salida en modo binario (\"wb\") dentro del directorio data_dir con el nombre definido en gesture_data \n",
    "        (p.ej. \"gesture_data_v15.pkl\"). Usa un bloque with para asegurar el cierre automático del archivo.\"\"\"\n",
    "        pickle.dump(data_to_save, f) #Serializa el diccionario data_to_save y lo escribe en el archivo abierto. De este modo se guarda todo el dataset de manera persistente.\n",
    "    set_message(f\"Datos guardados: {len(data)} muestras\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1258d",
   "metadata": {},
   "source": [
    "### RECOLECCION DE MUESTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b898ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sample(landmarks): #Define una función que recibe como parámetro landmarks, un vector de características (por ejemplo, 126 valores extraídos de los puntos de MediaPipe + distancias + ángulos) correspondiente a un frame.\n",
    "    global is_collecting, samples_collected, last_sample_time, data, labels\n",
    "    \n",
    "    if not is_collecting: #i no estamos en modo recolección, abortamos la función inmediatamente y devolvemos False, ya que no corresponde capturar nada.\n",
    "        return False\n",
    "    \n",
    "    current_time = time.time() #Captura el timestamp actual (en segundos desde epoch) para controlar la frecuencia de muestreo.\n",
    "    if current_time - last_sample_time >= sample_delay: #5.\tComprueba si ha pasado el tiempo mínimo entre capturas (por defecto 50 ms). Esto evita capturar muestras demasiado rápido que sean casi idénticas.\n",
    "        data.append(landmarks) #agrega el vector actual a la lista global de muestras.\n",
    "        labels.append(current_gesture) #guarda la etiqueta textual correspondiente a esta muestra.\n",
    "\n",
    "        #Incrementa el contador de muestras recolectadas y actualiza el tiempo del último muestreo exitoso.\n",
    "        samples_collected += 1\n",
    "        last_sample_time = current_time\n",
    "        \n",
    "        if samples_collected % max_samples == 0:\n",
    "            save_data()\n",
    "        \"\"\"Si se ha llegado a un múltiplo de max_samples, se llama a save_data() para persistir lo recolectado.\n",
    "        (Aunque este uso parece redundante con el bloque siguiente: probablemente está pensado por seguridad si no se usa stop_collection.)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if samples_collected >= max_samples:\n",
    "            stop_collection()\n",
    "            return True\n",
    "        \"\"\"Si se alcanzó el número máximo de muestras para esta seña:\n",
    "        o\tSe detiene la recolección automáticamente con stop_collection().\n",
    "        o\tDevuelve True para avisar al flujo principal que se completó la captura.\n",
    "        \"\"\"\n",
    "    \n",
    "    return False #Si no se cumplen las condiciones para guardar o finalizar, retorna False, lo que indica que la captura continúa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad36f67",
   "metadata": {},
   "source": [
    "¿Qué hace en sí esta función?\n",
    "\n",
    "•Controla la recolección temporizada de vectores de gestos.\n",
    "\n",
    "•Guarda cada muestra con su etiqueta correspondiente.\n",
    "\n",
    "•Evita sobrecargar el dataset con datos repetidos por estar demasiado próximos en el tiempo.\n",
    "\n",
    "•Finaliza automáticamente cuando se completa el cupo por gesto.\n",
    "\n",
    "•Permite al flujo principal mantenerse simple: solo tiene que llamar continuamente collect_sample(...) y revisar si devolvió True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3435cd",
   "metadata": {},
   "source": [
    "### CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48579101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(): #Declara la función load_data sin parámetros, que intentará leer el dataset serializado de gestos desde el sistema de archivos.\n",
    "    global data, labels\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            \"\"\"Abre el archivo cuyo nombre combina la ruta data_dir y el nombre gesture_data (por ejemplo \n",
    "            \"hand_gestures_data_v15/gesture_data_v15.pkl\") en modo de lectura binaria (\"rb\"). \n",
    "            El bloque with garantiza el cierre automático del archivo.\"\"\"\n",
    "             \n",
    "            loaded_data = pickle.load(f) #Usa pickle para deserializar el contenido del archivo. Se espera que sea un diccionario con las claves \"features\" y \"labels\".\n",
    "            data = loaded_data[\"features\"] #Extrae la lista de vectores de características del diccionario y la asigna a la variable global data.\n",
    "            labels = loaded_data[\"labels\"] #Extrae la lista de etiquetas correspondientes y la asigna a la variable global labels.\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos: {e}\")\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5e855",
   "metadata": {},
   "source": [
    "Con esto, la función load_data permite reutilizar datasets previos, acelerando el inicio del entrenamiento o evitando recolectar muestras desde cero cada vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a440d50",
   "metadata": {},
   "source": [
    "### RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_exists():\n",
    "    return os.path.exists(model_file) and os.path.exists(scaler_file) and os.path.exists(encoder_file)\n",
    "\n",
    "\n",
    "def create_neural_network(input_shape, num_classes):\n",
    "    \"\"\"Parámetros:\n",
    "    •\tinput_shape: cantidad de características que recibe cada vector de entrada (por ejemplo, 126 si usás 2 manos × 21 puntos × 3 coordenadas).\n",
    "    •\tnum_classes: cantidad total de señas diferentes que querés reconocer.\n",
    "    \"\"\"\n",
    "    #Se define un modelo secuencial, es decir, una pila lineal de capas conectadas una tras otra (sin bifurcaciones).\n",
    "    model = Sequential([  \n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)), \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3), \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(), \n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), #Algoritmo de optimización adaptativo. Aprende más rápido y de forma más estable que SGD.\n",
    "        #Tamaño de paso del optimizador. Ni muy grande (inestable), ni muy chico (lento).\n",
    "        loss='sparse_categorical_crossentropy', #Función de pérdida para clasificación multiclase con etiquetas enteras. Se aplica cuando las clases se codifican como 0, 1, 2, ..., y no como one-hot.\n",
    "        metrics=['accuracy'] #Mide qué porcentaje de predicciones coincide con la etiqueta real.\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643decd",
   "metadata": {},
   "source": [
    "```python\n",
    "    model = Sequential([  \n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)), \n",
    "        \"\"\" •\tDense(64): Crea una capa completamente conectada con 64 neuronas.\n",
    "            •\tactivation='relu': Cada neurona aplica la función ReLU(x) = max(0, x)\n",
    "                    Introduce no linealidad y ayuda a que la red aprenda relaciones complejas.\n",
    "            •\tinput_shape=(input_shape,): Define la dimensión de entrada.\n",
    "                    Por ejemplo, si cada vector tiene 126 características, entonces recibe una tupla (126,).\n",
    "            •\tkernel_regularizer=l2(0.001): Aplica regularización L2 (también llamada “weight decay”) para penalizar pesos grandes y evitar el sobreajuste:\n",
    "            el calculo seria: perdida total=cross-entropy + lamda*sum(w^2), en este caso lamda=0.001.\n",
    "            \"\"\"\n",
    "\n",
    "        BatchNormalization(),\n",
    "        \"\"\" •\tEsta capa normaliza las salidas de la capa anterior (media ≈ 0, varianza ≈ 1) para estabilizar y acelerar el entrenamiento.\n",
    "            •\tCalcula, por cada mini-batch, media y desviación estándar de las salidas de las 64 neuronas anteriores.\n",
    "            •\tTambién permite que la red aprenda una escala y un sesgo nuevos, si es útil.\n",
    "            \"\"\"\n",
    "      \n",
    "        Dropout(0.3),\n",
    "        \"\"\" •\tDesactiva aleatoriamente el 30% de las neuronas de esta capa durante el entrenamiento.\n",
    "            •\tEvita que la red dependa demasiado de ciertas neuronas (sobreajuste) y fuerza a que aprenda representaciones más generales.\n",
    "            \"\"\"\n",
    "        \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        \"\"\" •\tCapa totalmente conectada de 32 neuronas con activación ReLU.\n",
    "            •\tTambién tiene regularización L2.\n",
    "            •\tActúa como transformación intermedia para reducir la dimensionalidad y extraer patrones más abstractos.\n",
    "            \"\"\"\n",
    "        BatchNormalization(), #Aplica normalización a las 32 neuronas.\n",
    "\n",
    "        Dropout(0.2), #desactiva aleatoriamente el 20% de ellas en cada batch de entrenamiento.\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "        \"\"\" •\tCapa de salida con tantas neuronas como clases (señas distintas).\n",
    "            •\tactivation='softmax' transforma las salidas en probabilidades:\n",
    "                yk=e^zk / Σe^zi\n",
    "                donde zk es la salida de la k-ésima neurona y Σe^zi es la suma de todas las exponenciales de las salidas.\n",
    "            •\tEsto asegura que todas las salidas sumen 1, permitiendo interpretarlas como probabilidades de pertenencia a cada clase.\n",
    "            •   Cada neurona da la probabilidad estimada de que el vector de entrada corresponda a una seña específica.\n",
    "                \"\"\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75171f16",
   "metadata": {},
   "source": [
    "¿Cómo funciona en conjunto?\n",
    "1.\tEntrás con un vector de características numéricas (por ejemplo, 126 valores).\n",
    "2.\tLas capas densas lo transforman y lo comprimen, buscando patrones en los movimientos de las manos.\n",
    "3.\tSe regulariza y normaliza para que aprenda sin sobreajustarse.\n",
    "4.\tLa capa softmax da una probabilidad para cada seña conocida.\n",
    "5.\tEl modelo aprende ajustando los pesos para que la salida se acerque cada vez más a la etiqueta correcta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79c92d",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global model, scaler, label_encoder, metrics, is_trained\n",
    "    \"\"\" o\tmodel: contendrá la red entrenada.\n",
    "        o\tscaler: el objeto StandardScaler.\n",
    "        o\tlabel_encoder: el LabelEncoder.\n",
    "        o\tmetrics: diccionario donde guardamos precisión y tiempo.\n",
    "        o\tis_trained: flag que indica si ya hay un modelo listo.\n",
    "        \"\"\"\n",
    "    \n",
    "    #Validación mínima de datos\n",
    "    if len(data) < 10:\n",
    "        set_message(\"Se necesitan más datos para entrenar\", 2)\n",
    "        return False\n",
    "    \"\"\"\n",
    "        o\tSi hemos recolectado menos de 10 muestras en total, mostramos un mensaje (“Se necesitan más datos...”) y abortamos el entrenamiento devolviendo False.\n",
    "        o\tEsto evita errores de shape y de entrenamiento sobre conjuntos demasiado pequeños.\n",
    "        \"\"\"\n",
    "    #Construcción de matrices de entrada y salida\n",
    "    X = np.array(data) #array de forma (N, F), con N muestras y F características (landmarks+distancias+ángulos).\n",
    "    y = np.array(labels) #vector de forma (N,) con etiquetas textuales para cada muestra.\n",
    "    \n",
    "    # Codificar etiquetas\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \"\"\" o\tfit_transform ajusta internamente el LabelEncoder al conjunto y, generando un array de enteros y_encoded en {0,...,C-1}.\n",
    "        o\tLa asociación texto→índice se guarda en label_encoder.classes_.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \"\"\" o \ttest_size=0.2: reserva el 20% de las muestras para evaluación final.\n",
    "        o\trandom_state=42: semilla fija para reproducibilidad.\n",
    "        o\tstratify=y_encoded: asegura que la proporción de cada clase en train/test sea la misma, evitando sesgo si algunas señas son minoritarias.\n",
    "        \"\"\"\n",
    "    \n",
    "    # Normalizar datos / Garantiza que la distribución de entradas sea homogénea, mejorando la convergencia de la red.\n",
    "    X_train = scaler.fit_transform(X_train) #calcula medias μj y desviaciones σj en cada característica j, y aplica media de xj = (xj - μj) / σj a cada vector de entrenamiento.\n",
    "    X_test = scaler.transform(X_test) # usa las mismas medias y desviaciones calculadas en el entrenamiento para normalizar el conjunto de test.\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    num_classes = len(set(y_encoded)) #número de clases distintas (tamaño de la salida softmax).\n",
    "    set_message(f\"Entrenando modelo con {num_classes} clases...\", 2)\n",
    "    \n",
    "    #creacion de la red neuronal\n",
    "    model = create_neural_network(X_train.shape[1], num_classes)\n",
    "    \"\"\" o\tLlama a create_neural_network(input_shape=X_train.shape[1], num_classes).\n",
    "        o\tDefine la arquitectura multicapa con regularización, normalización y dropout.\n",
    "        \"\"\"\n",
    "    \n",
    "    #Callbacks para mejorar el entrenamiento\n",
    "    callbacks = [ \n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True), #detiene el entrenamiento si la pérdida de validación (val_loss) no mejora tras 10 épocas, y restaura los pesos de la mejor época.\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001) #reduce la tasa de aprendizaje a la mitad (factor=0.5) si val_loss no mejora tras 5 épocas, con un mínimo de 1e-4.\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time() #\tGuarda el timestamp de inicio para calcular luego la duración total del entrenamiento.\n",
    "    \n",
    "    history = model.fit( # Entrenamiento del modelo\n",
    "        X_train, y_train, # X_train: matriz de entrenamiento, y_train: etiquetas codificadas.\n",
    "        epochs=50, #número máximo de pasadas completas sobre el set de entrenamiento.\n",
    "        batch_size=32, #cada actualización de pesos se hace con 32 muestras.\n",
    "        validation_split=0.2, #reserva un 20% de X_train para validación interna en cada época.\n",
    "        callbacks=callbacks, #detiene o ajusta lr según el rendimiento en validación.\n",
    "        verbose=1 #muestra barra de progreso y métricas por época.\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time #o\tResta el timepo actual con start_time para obtener segundos totales de entrenamiento.\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1) #devuelve un array (N_test, num_classes) de probabilidades. selecciona la clase con mayor probabilidad para cada muestra.\n",
    "    accuracy = accuracy_score(y_test, y_pred) #calcula la proporción de predicciones correctas.\n",
    "    \n",
    "    # Guardar métricas \n",
    "    metrics['accuracy'] = accuracy #precisión en prueba.\n",
    "    metrics['val_accuracy'] = max(history.history['val_accuracy']) #mejor precisión observada en validación interna (history.history['val_accuracy']).\n",
    "    metrics['training_time'] = training_time #duración total.\n",
    "    \n",
    "    # Guardar modelo y preprocesadores\n",
    "    model.save(model_file) #guarda arquitectura y pesos en HDF5.\n",
    "    with open(scaler_file, 'wb') as f:\n",
    "        pickle.dump(scaler, f) #persiste el objeto StandardScaler.\n",
    "    with open(encoder_file, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)#persiste el LabelEncoder.\n",
    "    #Estos archivos permitirán luego cargar el pipeline completo sin reentrenar.\n",
    "    \n",
    "    set_message(f\"Modelo entrenado con precisión: {accuracy:.2%}\", 3) #o\tMuestra un mensaje final con la precisión de prueba (p. ej. “Modelo entrenado con precisión: 94.50%”).\n",
    "    is_trained = True\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837e8c4",
   "metadata": {},
   "source": [
    "🔍 Resumen de la lógica\n",
    "1.\tVerifica que haya datos suficientes.\n",
    "2.\tConstruye matrices NumPy de datos y etiquetas.\n",
    "3.\tCodifica etiquetas a enteros.\n",
    "4.\tDivide en entrenamiento y prueba de forma estratificada.\n",
    "5.\tNormaliza entradas según StandardScaler.\n",
    "6.\tCrea la red neuronal con arquitectura definida.\n",
    "7.\tEntrena con validación interna, early stopping y reducción dinámica del learning rate.\n",
    "8.\tEvalúa en datos de prueba externos y calcula precisión.\n",
    "9.\tGuarda el modelo y los preprocesadores para uso futuro.\n",
    "10.\tInforma al usuario y actualiza el estado interno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d1ad",
   "metadata": {},
   "source": [
    "### CARGA DEL MODELO ENTRENADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(): #Define la función encargada de cargar desde disco el modelo entrenado y los preprocesadores.\n",
    "    global scaler, label_encoder\n",
    "    try:\n",
    "        model = load_model(model_file) #Utiliza la función de Keras load_model para leer el archivo HDF5 (hand_gesture_nn_model_v15.h5) y reconstruir la red neuronal completa con sus pesos.\n",
    "        with open(scaler_file, 'rb') as f: #Abre el archivo que contiene el objeto StandardScaler serializado (hand_gesture_scaler_v15.pkl) en modo lectura binaria.\n",
    "            scaler = pickle.load(f) #Deserializa el StandardScaler con las medias y desviaciones aprendidas en el entrenamiento, y lo asigna a la variable global scaler.\n",
    "        with open(encoder_file, 'rb') as f: #Abre el archivo del LabelEncoder (hand_gesture_encoder_v15.pkl).\n",
    "            label_encoder = pickle.load(f) #Deserializa el encoder que mapea índices numéricos a etiquetas de texto, y lo asigna a la variable global label_encoder.\n",
    "        set_message(\"Modelo cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {e}\")\n",
    "        set_message(\"Error al cargar el modelo\", 2)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651eed",
   "metadata": {},
   "source": [
    "### MODELO DE TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5be08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.5): #que toma un vector de landmarks y devuelve la etiqueta predicha junto con la confianza, usando el modelo TFLite y los preprocesadores\n",
    "    \"\"\"Declara la función que realiza la inferencia.\n",
    "        o\tlandmarks: lista o array 1D de características (coordenadas, distancias, ángulos).\n",
    "        o\ttflite_model: instancia de la clase TFLiteModel preparada para inferir.\n",
    "        o\tscaler: objeto StandardScaler ya ajustado.\n",
    "        o\tlabel_encoder: objeto LabelEncoder con el mapeo inverso.\n",
    "        o\tthreshold=0.5: confianza mínima para aceptar una predicción; por debajo se marca como desconocido.\n",
    "        \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Preprocesar los landmarks\n",
    "        landmarks_array = np.array(landmarks).reshape(1, -1) #añade la dimensión de batch; si landmarks tenía longitud F, ahora es (1, F).\n",
    "        landmarks_scaled = scaler.transform(landmarks_array) #Normalizar usando el StandardScaler aplica media de xj = (xj - μj) / σj, transformando la muestra individual al mismo espacio que el entrenamiento.\n",
    "        \n",
    "        # Realizar predicción\n",
    "        predictions = tflite_model.predict(landmarks_scaled)[0] #devuelve un array (1, C) de probabilidades para cada clase.\t[0] extrae el vector de tamaño C\n",
    "        \n",
    "        # Obtener la clase con mayor probabilidad \n",
    "        max_prob_idx = np.argmax(predictions) #índice de la probabilidad máxima\n",
    "        confidence = predictions[max_prob_idx] #el valor de esa probabilidad, entre 0 y 1.\n",
    "        \n",
    "\n",
    "        #Umbral de confianza:\n",
    "        if confidence >= threshold:\n",
    "            # Decodificar la etiqueta\n",
    "            predicted_label = label_encoder.inverse_transform([max_prob_idx])[0]\n",
    "            return predicted_label, confidence\n",
    "            \"\"\" o\tSolo aceptamos la predicción si la probabilidad supera threshold (por defecto 0.5).\n",
    "                o\tEsto evita emisiones erráticas cuando el modelo no está seguro. \n",
    "\n",
    "               8.\tDecodificar índice a texto:\n",
    "                o\tinverse_transform([max_prob_idx]) convierte el índice de clase en la cadena de la seña (ej. \"HOLA\").\n",
    "                o\tLa función retorna una tupla (etiqueta, confianza). \"\"\"  \n",
    "        else:\n",
    "            return \"Desconocido\", confidence #Si la confianza es baja, devolvemos \"Desconocido\" y la confianza obtenida. Esto permite manejar gestos no entrenados o detecciones dudosas\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la predicción: {e}\")\n",
    "        return \"Error\", 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4367d",
   "metadata": {},
   "source": [
    "¿Cómo encaja esta función?\n",
    "•\tSe llama en el bucle de inferencia en vivo (me_15_v6_evaluacion.ipynb) tras extraer y normalizar los landmarks.\n",
    "•\tDevuelve un string con la seña detectada (o \"Desconocido\"/\"Error\") y su nivel de confianza.\n",
    "•\tCon esa información se decide si reproducir la salida por audio y mostrar el texto en pantalla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b02471",
   "metadata": {},
   "source": [
    "### CONVERSION A TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_file, model_tflite): #toma un modelo Keras y lo convierte a formato TensorFlow Lite\n",
    "    \"\"\" o\tmodel_file: ruta al archivo HDF5 del modelo Keras entrenado.\n",
    "        o\tmodel_tflite: ruta de salida donde se guardará el .tflite.\n",
    "        \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(model_file):\n",
    "            raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "        \n",
    "        # Cargar el modelo entrenado\n",
    "        modelo = load_model(model_file)\n",
    "        \"\"\"Carga del modelo Keras\n",
    "        o\tUsa load_model de Keras para leer model_file y reconstruir la arquitectura y pesos.\n",
    "        o\tSe asigna a la variable local modelo.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convertir a TensorFlow Lite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(modelo) #Crea un objeto TFLiteConverter a partir del modelo Keras cargado.\n",
    "        tflite_model = converter.convert() #Llama a converter.convert(), que traduce la red y sus pesos al formato optimizado de TensorFlow Lite, devolviendo un bytes con el modelo TFLite\n",
    "        \n",
    "        # Guardar el modelo convertido\n",
    "        with open(model_tflite, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al convertir el modelo a TFLite:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f2d65",
   "metadata": {},
   "source": [
    "### ELIMINADOR DE SEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_gesture(target_label): #Declara la función que recibe target_label, la etiqueta (nombre de seña) que se desea eliminar.\n",
    "    global data, labels\n",
    "    if target_label not in labels: #Comprueba si la etiqueta existe en la lista de etiquetas.\n",
    "        print(f\"Error: La etiqueta '{target_label}' no existe\") #Si no se encuentra, muestra un mensaje de error y retorna False.\n",
    "        return False\n",
    "    \n",
    "    # Filtrar elementos a mantener\n",
    "    new_data = [] #Lista temporal para almacenar los vectores de características que no corresponden a la etiqueta a eliminar\n",
    "    new_labels = [] #Lista temporal para almacenar las etiquetas que no corresponden a la etiqueta a eliminar\n",
    "    deleted_count = 0 #Contador de cuántas muestras se eliminaron\n",
    "    \n",
    "    for feature, label in zip(data, labels): #Itera sobre las listas data y labels simultáneamente, emparejando cada vector de características con su etiqueta correspondiente.\n",
    "        if label == target_label: #Si la etiqueta coincide con la que se desea eliminar:\n",
    "            deleted_count += 1 # Incrementa el contador de eliminaciones\n",
    "        else:\n",
    "            new_data.append(feature) # Agrega el vector de características a la nueva lista si no es la etiqueta objetivo\n",
    "            new_labels.append(label) # Agrega la etiqueta a la nueva lista si no es la etiqueta objetivo\n",
    "    \n",
    "    # Actualizar listas globales\n",
    "    data.clear() # Limpia las listas globales data y labels para evitar duplicados o residuos de la eliminación.\n",
    "    labels.clear() # Limpia las etiquetas globales\n",
    "    data.extend(new_data) # Agrega los vectores de características filtrados a la lista global data\n",
    "    labels.extend(new_labels) # Agrega las etiquetas filtradas a la lista global labels\n",
    "    \n",
    "    print(f\"Se eliminaron {deleted_count} muestras de '{target_label}'\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909a1bc",
   "metadata": {},
   "source": [
    "### GENERADOR DE AUDIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "audio_dir = \"pyttsx3_audios\" #carpeta donde se guardarán los audios .wav generados.\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "# Configurar motor TTS\n",
    "engine = pyttsx3.init() #inicializa el motor de Text-To-Speech (síntesis de voz offline).\n",
    "engine.setProperty('rate', 150) #velocidad de habla (palabras por minuto).\n",
    "engine.setProperty('volume', 1.0)  # volumen máximo (rango de 0.0 a 1.0).\n",
    "\n",
    "def trim_audio_silence(file_path):\n",
    "    \"\"\"Recorta silencios al inicio y final del audio\"\"\"\n",
    "    audio = AudioSegment.from_file(file_path, format=\"wav\")\n",
    "\n",
    "    # Parámetros ajustables\n",
    "    config = {\n",
    "        'min_silence_len': 200,     # 200 ms de silencio mínimo para considerar corte\n",
    "        'silence_thresh': -45,      # -45 dB de umbral de silencio\n",
    "        'end_buffer': 150           # 150 ms extra al final\n",
    "    }\n",
    "    \n",
    "    # Detectar segmentos no silenciosos\n",
    "    nonsilent_parts = detect_nonsilent(\n",
    "        audio,\n",
    "        min_silence_len=config['min_silence_len'], # Duración mínima de silencio a considerar (ms)\n",
    "        silence_thresh=config['silence_thresh'] # Umbral de volumen para considerar silencio (dB)\n",
    "    )\n",
    "    \n",
    "    if nonsilent_parts:\n",
    "        start = max(0, nonsilent_parts[0][0] - 50)  # 50 ms buffer inicial\n",
    "        end = nonsilent_parts[-1][1] + config['end_buffer']\n",
    "        trimmed_audio = audio[start:end]\n",
    "\n",
    "        # --- AÑADIDO: subir el volumen  ---\n",
    "        gain_db = 24  # ganancia en db\n",
    "        trimmed_audio = trimmed_audio + gain_db\n",
    "\n",
    "        trimmed_audio.export(file_path, format=\"wav\")\n",
    "\n",
    "def compilador_audios(label):\n",
    "    \"\"\"Genera y ajusta audio para eliminar silencios\"\"\"\n",
    "    nombre_archivo = label.replace(' ', '_').lower() + '.wav'\n",
    "    ruta_audio = os.path.join(audio_dir, nombre_archivo)\n",
    "    \n",
    "    if os.path.exists(ruta_audio):\n",
    "        return\n",
    "    \n",
    "    temp_path = os.path.join(audio_dir, \"temp.wav\")\n",
    "    try:\n",
    "        # Generar audio temporal\n",
    "        engine.save_to_file(label, temp_path)\n",
    "        engine.runAndWait()\n",
    "        \n",
    "        # Recortar y renombrar\n",
    "        trim_audio_silence(temp_path)\n",
    "        os.rename(temp_path, ruta_audio)\n",
    "        print(f\"Audio generado: {nombre_archivo}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generando {label}: {str(e)}\")\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "def generar_audios():\n",
    "    \"\"\"Genera audios para todas las etiquetas únicas\"\"\"\n",
    "    etiquetas_unicas = set(labels)\n",
    "    print(\"\\nGenerando audios para señas...\")\n",
    "    \n",
    "    for label in etiquetas_unicas:\n",
    "        compilador_audios(label)\n",
    "    \n",
    "    print(\"Proceso de generación de audios completado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd15d98",
   "metadata": {},
   "source": [
    "### MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89e8856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_menu():\n",
    "    print(\"\\n=== MENU PRINCIPAL ===\")\n",
    "    print(\"1. Recolectar nueva seña\")\n",
    "    print(\"2. Entrenar modelo\")\n",
    "    print(\"3. Listar señas cargadas\")\n",
    "    print(\"4. Eliminar señas\")\n",
    "    print(\"5. Generar Audios\")\n",
    "    print(\"0. Salir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c02105",
   "metadata": {},
   "source": [
    "### LISTADO DE GESTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81a424fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gestures():\n",
    "    # Asumiendo que 'labels' es la lista donde se guardan las señas\n",
    "    if not labels:\n",
    "        print(\"No hay señas guardadas.\")\n",
    "    else:\n",
    "        unique_gestures = list(set(labels))\n",
    "        print(\"\\n--- Señas Guardadas ---\")\n",
    "        for i, gesture in enumerate(unique_gestures, 1):\n",
    "            print(f\"{i}. {gesture}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7bd27",
   "metadata": {},
   "source": [
    "### RECOLECCION DE SEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collection_mode(): #1.\tDefine la función principal que se ejecuta cuando el sistema entra en modo \"recolectar nuevas señas\".\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera() #Intenta instanciar el objeto UDPCamera, que recibe frames de video vía UDP desde la Raspberry Pi.\n",
    "        print(\"Cámara UDP iniciada para recolección.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    while is_collecting:  # Este bucle se mantiene activo mientras is_collecting == True, lo cual se establece al llamar a start_collection().\n",
    "        ret, frame = cap.read() #Intenta capturar un frame de video desde el socket UDP.\n",
    "        \n",
    "        if not ret: #Si no se pudo capturar un frame, imprime un mensaje de error y espera 100 ms antes de reintentar.\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame, send_sock)\n",
    "        \"\"\"8.\tLlama a la función de detección y extracción de puntos de mano.\n",
    "                o\tlandmarks: vector numérico de características.\n",
    "                o\thands_detected: indica si hay al menos una mano en el frame.\n",
    "                o\tsend_sock: permite enviar coordenadas de muñeca vía UDP a la Raspberry.\n",
    "                \"\"\"\n",
    "        frame_h, frame_w, _ = frame.shape #Extrae alto y ancho de la imagen para colocar textos y gráficos en pantalla.\n",
    "\n",
    "        # Mostrar información en pantalla durante la recolección\n",
    "        progress = int((samples_collected / max_samples) * frame_w)\n",
    "        cv2.rectangle(frame, (0, 0), (progress, 20), (0, 255, 0), -1) #Calcula el progreso de muestras recolectadas y dibuja una barra verde en la parte superior de la imagen.\n",
    "\n",
    "        cv2.putText(frame, f\"Recolectando: {current_gesture} ({samples_collected}/{max_samples})\", \n",
    "                    (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2) #Muestra en pantalla el nombre de la seña y la cantidad de muestras recolectadas hasta el momento.\n",
    "       \n",
    "       \n",
    "        #Detección de manos y recolección\n",
    "        if hands_detected: #Si hay manos en el frame, se intenta capturar la muestra actual (pasando los landmarks a collect_sample()).\n",
    "            collect_sample(landmarks)\n",
    "        else:\n",
    "            cv2.putText(frame, \"¡Muestra las manos!\", (frame_w//2 - 100, frame_h//2), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "        if not is_collecting:  # Cuando termina la recolección\n",
    "            menu_active = True\n",
    "            save_data()\n",
    "            \"\"\"Si en algún momento is_collecting se pone en False (por ejemplo, al completar las max_samples), \n",
    "            se activa el menú principal (menu_active) y se guardan los datos recolectados en disco.\"\"\"\n",
    "        \n",
    "        cv2.imshow(\"Recolectar Señas\", frame) #Muestra el frame con superposición de información y gráficos en una ventana de OpenCV.\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        # Pulsar esc para salir de la recolección.\n",
    "        if key == 27:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed8152",
   "metadata": {},
   "source": [
    "¿Qué hace esta función?\n",
    "•\tMuestra en tiempo real los frames con indicaciones.\n",
    "•\tCaptura muestras automáticamente mientras detecta una mano.\n",
    "•\tGuía visualmente al usuario en la recolección.\n",
    "•\tGuarda todo al completar o al presionar ESC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b50ac",
   "metadata": {},
   "source": [
    "### FUNCION PRINCIPAL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae199e",
   "metadata": {},
   "source": [
    "Esta es la interfaz central donde el usuario puede recolectar, entrenar, gestionar y generar audios para el sistema de reconocimiento de gestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ff93c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "    \n",
    "    # Inicialización del sistema\n",
    "    is_trained = False\n",
    "    model = None\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Mostrar el menú en la consola\n",
    "    print_menu()\n",
    "\n",
    "    # Bucle principal de selección en consola\n",
    "    while True:\n",
    "        opcion = input(\"\\nSelecciona una opción (Recolectar: 1, Entrenar: 2, Listar: 3, Eliminar: 4, Gen.Audio: 5, Salir: 0): \").strip()\n",
    "        \n",
    "        if opcion == '1':\n",
    "            # Recolección de señas\n",
    "            gesture_name = input(\"Ingrese nombre de la seña (ej. 'Hola'): \")\n",
    "            if gesture_name:\n",
    "                start_collection(gesture_name)\n",
    "                # Iniciar la cámara para mostrar video durante la recolección\n",
    "                run_collection_mode()\n",
    "                \n",
    "        elif opcion == '2':\n",
    "            if len(data) > 10:\n",
    "                train_model()\n",
    "                model = load_saved_model() if check_model_exists() else None\n",
    "                is_trained = True\n",
    "                print(\"Entrenamiento completado. Modelo entrenado.\")\n",
    "                convert_to_tflite(model_file, model_tflite)\n",
    "                print(\"Convertido a TFLite para evaluación en tiempo real\")\n",
    "            else:\n",
    "                print(\"¡Necesitas al menos 10 muestras para entrenar!\")\n",
    "                \n",
    "        elif opcion == '3':\n",
    "            list_gestures()  # Lista las señas cargadas\n",
    "\n",
    "        elif opcion == '4':\n",
    "            if not labels:\n",
    "                print(\"No hay señas guardadas para eliminar\")\n",
    "                exit()\n",
    "            print(\"\\n--- Señas Registradas ---\")\n",
    "            for label in set(labels):\n",
    "                print(f\"- {label}\")\n",
    "            target_label = input(\"\\nIngrese el nombre exacto de la seña a eliminar: \").strip()\n",
    "            if delete_gesture(target_label):\n",
    "                save_data()\n",
    "            else:\n",
    "                print(\"No se realizaron cambios en los datos\")\n",
    "        \n",
    "        elif opcion == '5':\n",
    "            if not labels:\n",
    "                print(\"No hay señas guardadas\")\n",
    "                exit()\n",
    "            # Generar audios automáticamente\n",
    "            generar_audios()\n",
    "            \n",
    "\n",
    "                \n",
    "        elif opcion == '0':\n",
    "            print(\"Saliendo del programa...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Opción inválida, intenta nuevamente.\")\n",
    "        \n",
    "        # Mostrar nuevamente el menú luego de finalizar la opción seleccionada.\n",
    "        print_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbcb2f",
   "metadata": {},
   "source": [
    "# EJECUTAR PROGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40955799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "\n",
      "Generando audios para señas...\n",
      "Audio generado: abecedario.wav\n",
      "Proceso de generación de audios completado\n",
      "\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "Saliendo del programa...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    save_data()  # Guarda los datos recolectados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
