{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd78588",
   "metadata": {},
   "source": [
    "# ------------- PROYECTO FINAL G & S--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e16c",
   "metadata": {},
   "source": [
    "## LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Comunicación y cámara\n",
    "import socket\n",
    "import queue\n",
    "\n",
    "# Voz\n",
    "import pyttsx3\n",
    "import threading\n",
    "from collections import deque\n",
    "\n",
    "# Módulos para reconocimiento de voz\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import io\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd329ca",
   "metadata": {},
   "source": [
    "## UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0df331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para el reconocimiento de voz\n",
    "SAMPLE_RATE_IN = 48000  # Tasa del micrófono INMP441\n",
    "SAMPLE_RATE_OUT = 16000  # Tasa requerida por la API de reconocimiento\n",
    "BUFFER_DURATION = 5  # segundos\n",
    "\n",
    "\n",
    "UDP_IP_PI = \"192.168.7.2\"  # IP de la Raspberry Pi\n",
    "UDP_OPEN = '0.0.0.0'\n",
    "\n",
    "# Puertos para diferentes servicios\n",
    "UDP_PORT_MICROFONO = 5006\n",
    "UDP_PORT_TEXT = 5005\n",
    "UDP_PORT_SERVO = 5001  # Puerto para enviar comandos\n",
    "UDP_PORT_PARLANTE = 5003\n",
    "UDP_PORT_CAM = 5002  # Puerto para recibir video\n",
    "\n",
    "# MSS\n",
    "MAX_PACKET_SIZE = 1460  # Tamaño máximo del paquete UDP\n",
    "\n",
    "# Buffer UDP\n",
    "BUFFER_UDP = 65536 #16 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9c09ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar al inicio del archivo\n",
    "UDP_PORT_HANDS_STATUS = 5007  # Nuevo puerto\n",
    "sock_hands_status = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# ---- Agregar al inicio del archivo ----\n",
    "UDP_PORT_HEARTBEAT = 5008\n",
    "sock_heartbeat = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "73bbfefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para enviar heartbeat\n",
    "def send_heartbeat():\n",
    "    while True:\n",
    "        try:\n",
    "            sock_heartbeat.sendto(b\"HEARTBEAT\", (UDP_IP_PI, UDP_PORT_HEARTBEAT))\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            break  # Salir si hay error\n",
    "\n",
    "# Iniciar hilo de heartbeat al comenzar el programa\n",
    "heartbeat_thread = threading.Thread(target=send_heartbeat, daemon=True)\n",
    "heartbeat_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f43af6",
   "metadata": {},
   "source": [
    "## VOZ A PANTALLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c1645",
   "metadata": {},
   "source": [
    "### RECONOCEDOR DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7a852357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el reconocedor de voz\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Configuración UDP para voz\n",
    "sock_voice = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "microfono_queue = queue.Queue()\n",
    "\n",
    "# Variable para controlar el servicio de reconocimiento de voz\n",
    "speech_recognition_running = True #VA EN EL WHILE.\n",
    "\n",
    "# Variable para almacenar la última transcripción\n",
    "last_transcription = \"\" #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "\n",
    "def recibir_audio():\n",
    "    sock_audio = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    try:\n",
    "        sock_audio.bind((\"0.0.0.0\", UDP_PORT_MICROFONO))\n",
    "        \n",
    "        buffer = bytearray()\n",
    "        bytes_needed = SAMPLE_RATE_IN * 4 * BUFFER_DURATION  # 4 bytes por muestra (32-bit)\n",
    "        \n",
    "        while speech_recognition_running:\n",
    "            data, _ = sock_audio.recvfrom(4096)\n",
    "            buffer.extend(data)\n",
    "            \n",
    "            while len(buffer) >= bytes_needed:\n",
    "                # Extraer 5 segundos de audio\n",
    "                chunk = bytes(buffer[:bytes_needed])\n",
    "                del buffer[:bytes_needed]\n",
    "                \n",
    "                # Convertir a formato numpy\n",
    "                audio_int32 = np.frombuffer(chunk, dtype=np.int32)\n",
    "                audio_float32 = audio_int32.astype(np.float32) / 2**31\n",
    "                \n",
    "                # Remuestrear a 16kHz\n",
    "                audio_16k = librosa.resample(\n",
    "                    audio_float32,\n",
    "                    orig_sr=SAMPLE_RATE_IN,\n",
    "                    target_sr=SAMPLE_RATE_OUT\n",
    "                )\n",
    "                \n",
    "                # Convertir a int16 para la API de reconocimiento\n",
    "                audio_int16 = (audio_16k * 32767).astype(np.int16)\n",
    "                \n",
    "                # Crear un archivo WAV en memoria\n",
    "                wav_buffer = io.BytesIO()\n",
    "                with wave.open(wav_buffer, 'wb') as wav_file:\n",
    "                    wav_file.setnchannels(1)  # Mono\n",
    "                    wav_file.setsampwidth(2)  # 2 bytes por muestra (16 bits)\n",
    "                    wav_file.setframerate(SAMPLE_RATE_OUT)\n",
    "                    wav_file.writeframes(audio_int16.tobytes())\n",
    "                \n",
    "                wav_buffer.seek(0)  # Rebobinar el buffer\n",
    "                microfono_queue.put(wav_buffer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en recibir_audio: {e}\")\n",
    "    #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY  ver de comentarlo\n",
    "    finally: \n",
    "        sock_audio.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5e666",
   "metadata": {},
   "source": [
    "### PROCESAR AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e0f2738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_audio():\n",
    "    global last_transcription #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "    while speech_recognition_running:\n",
    "        try:\n",
    "            wav_buffer = microfono_queue.get(timeout=1)\n",
    "            \n",
    "            # Crear un objeto AudioData desde el buffer WAV\n",
    "            with sr.AudioFile(wav_buffer) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "            \n",
    "            # Realizar la transcripción usando la API gratuita de Google\n",
    "            transcription = recognizer.recognize_google(audio_data, language=\"es-ES\")\n",
    "            \n",
    "            print(f\"Transcripción: {transcription}\")\n",
    "            last_transcription = transcription #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "            \n",
    "            # Enviar transcripción por UDP si es necesario\n",
    "            sock_voice.sendto(transcription.encode(), (UDP_IP_PI, UDP_PORT_TEXT))\n",
    "            \n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"No se detectó voz en el audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Error en la solicitud a la API de Google: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error en la transcripción: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687dfd",
   "metadata": {},
   "source": [
    "### ACTIVACION DE RECONOCIMIENTO DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fcdb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para iniciar el servicio de reconocimiento de voz\n",
    "def start_speech_recognition():\n",
    "    global speech_recognition_running\n",
    "    speech_recognition_running = True\n",
    "    \n",
    "    # Iniciar hilos para el reconocimiento de voz\n",
    "    audio_thread = threading.Thread(target=recibir_audio, daemon=True)\n",
    "    process_thread = threading.Thread(target=procesar_audio, daemon=True)\n",
    "    \n",
    "    audio_thread.start()\n",
    "    process_thread.start()\n",
    "    \n",
    "    print(\"Servicio de reconocimiento de voz iniciado...\")\n",
    "    return audio_thread, process_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67993b4d",
   "metadata": {},
   "source": [
    "### DETENCION DE RECONOCIMIENTO DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6cfb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para detener el servicio de reconocimiento de voz\n",
    "def stop_speech_recognition():\n",
    "    global speech_recognition_running\n",
    "    speech_recognition_running = False\n",
    "    sock_voice.close()\n",
    "    print(\"Servicio de reconocimiento de voz detenido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838398c2",
   "metadata": {},
   "source": [
    "## CAMARA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ceab1",
   "metadata": {},
   "source": [
    "### MOTOR TEXTO-VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "55a2514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: cuatro.wav (Duración: 0.63s)\n"
     ]
    }
   ],
   "source": [
    "# Configuración de directorios y archivos\n",
    "audio_dir = \"pyttsx3_audios\"\n",
    "\n",
    "# Socket UDP compartido\n",
    "udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# Variable global para última reproducción\n",
    "last_spoken_gesture = None\n",
    "\n",
    "# Cola thread-safe\n",
    "audio_queue = deque()\n",
    "queue_lock = threading.Lock()\n",
    "processing_event = threading.Event()\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    return text.replace(' ', '_').lower() + '.wav'\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    \"\"\"Obtiene duración precisa del archivo WAV\"\"\"\n",
    "    try:\n",
    "        with wave.open(file_path, 'r') as wav_file:\n",
    "            frames = wav_file.getnframes()\n",
    "            rate = wav_file.getframerate()\n",
    "            return frames / float(rate)\n",
    "    except:\n",
    "        return 0.1  # Duración por defecto si hay error\n",
    "\n",
    "def send_audio(file_path):\n",
    "    \"\"\"Envía archivo de audio por UDP sin delays intermedios\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            audio_data = f.read()\n",
    "        \n",
    "        # Envío rápido en chunks\n",
    "        total_chunks = (len(audio_data) + MAX_PACKET_SIZE - 1) // MAX_PACKET_SIZE\n",
    "        for i in range(total_chunks):\n",
    "            chunk = audio_data[i*MAX_PACKET_SIZE:(i+1)*MAX_PACKET_SIZE]\n",
    "            udp_socket.sendto(chunk, (UDP_IP_PI, UDP_PORT_PARLANTE))\n",
    "        \n",
    "        return get_audio_duration(file_path)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error enviando audio: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def queue_processor():\n",
    "    \"\"\"Procesa la cola de forma eficiente\"\"\"\n",
    "    while True:\n",
    "        processing_event.wait()  # Espera hasta que haya elementos\n",
    "        \n",
    "        with queue_lock:\n",
    "            if not audio_queue:\n",
    "                processing_event.clear()\n",
    "                continue\n",
    "            \n",
    "            text = audio_queue.popleft()\n",
    "            filename = sanitize_filename(text)\n",
    "            file_path = os.path.join(audio_dir, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            duration = send_audio(file_path)\n",
    "            print(f\"Audio enviado: {filename} (Duración: {duration:.2f}s)\")\n",
    "            time.sleep(duration)  # Espera exacta según duración real\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado: {filename}\")\n",
    "\n",
    "# Iniciar hilo procesador una sola vez\n",
    "processor_thread = threading.Thread(target=queue_processor, daemon=True)\n",
    "processor_thread.start()\n",
    "\n",
    "def speak_text(text):\n",
    "    \"\"\"Añade texto a la cola de reproducción\"\"\"\n",
    "    filename = sanitize_filename(text)\n",
    "    \n",
    "    with queue_lock:\n",
    "        # Evitar duplicados consecutivos\n",
    "        if not audio_queue or audio_queue[-1] != text:\n",
    "            audio_queue.append(text)\n",
    "            processing_event.set()  # Reactivar procesamiento si estaba inactivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb60538",
   "metadata": {},
   "source": [
    "### MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "afe0a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5, #probar con 0.4\n",
    "    min_tracking_confidence=0.5 #probar con 0.4\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14f25",
   "metadata": {},
   "source": [
    "### COMUNICACION CAMARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "97666dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = UDP_OPEN\n",
    "        self.port = UDP_PORT_CAM\n",
    "        self.buffer_size = BUFFER_UDP\n",
    "        self.mtu = MAX_PACKET_SIZE\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "                    if len(fragment) < self.mtu:  # Último fragmento\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        self.frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error en recepción: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()\n",
    "            return False, None\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cd388",
   "metadata": {},
   "source": [
    "### MODELO TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "fc0a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel:\n",
    "    def __init__(self, model_path):\n",
    "        # Cargar el modelo TFLite\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        # Obtener detalles de entrada y salida\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        # Asegurar el tipo de dato correcto y agregar dimensión batch si es necesario\n",
    "        input_data = np.array(input_data, dtype=self.input_details[0]['dtype'])\n",
    "        if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # Establecer la entrada y ejecutar la inferencia\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Obtener la salida\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "253769a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Configuración de TensorFlow para rendimiento en CPU\n",
    "    try:\n",
    "        # Verificar si hay GPU disponible (para futuras expansiones)\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        if physical_devices:\n",
    "            # Configuración para GPU (no se ejecutará en tu caso)\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(\"GPU disponible para aceleración\")\n",
    "        else:\n",
    "            # Optimización para CPU\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(4)  # Aprovecha núcleos físicos\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(2)  # Paralelismo entre operaciones\n",
    "            print(\"Modo CPU activado: Configuración optimizada para Intel Core i7-7500U\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error de configuración: {str(e)}\")\n",
    "        print(\"Usando configuración por defecto de CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f84a7",
   "metadata": {},
   "source": [
    "### ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "eb605c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_v15\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "model_file = \"hand_gesture_nn_model_v15.h5\"\n",
    "scaler_file = \"hand_gesture_scaler_v15.pkl\"\n",
    "encoder_file = \"hand_gesture_encoder_v15.pkl\"\n",
    "gesture_data = \"gesture_data_v15.pkl\" \n",
    "model_tflite = \"modelo_optimizadotl_v15.tflite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3a4ec0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False\n",
    "is_collecting = False\n",
    "current_gesture = \"\"\n",
    "samples_collected = 0\n",
    "max_samples = 5000\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0\n",
    "sample_delay = 0.05  # 50ms entre muestras\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\"\n",
    "message_until = 0\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ce453",
   "metadata": {},
   "source": [
    "### EXTRACCION DE LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d8867933",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hands_detected_time = 0\n",
    "hands_detected_status = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0cd498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # socket de los puntos de la muñeca\n",
    "\n",
    "def extract_hand_landmarks(frame, send_sock=None):\n",
    "    global last_hands_detected_time, hands_detected_status\n",
    "    \"\"\"\n",
    "    Extrae landmarks de las manos y realiza seguimiento de la muñeca derecha.\n",
    "    \n",
    "    Args:\n",
    "        frame: Imagen en formato BGR\n",
    "        send_sock: Socket UDP opcional para enviar datos de seguimiento\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (landmarks_data, hands_detected)\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    hands_detected = False\n",
    "    x_normalized = None\n",
    "    right_wrist_pixel = None\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        # Procesar ambas manos para landmarks\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Dibujar landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Extraer coordenadas\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "            landmarks_data.extend(landmarks)\n",
    "            \n",
    "            # Detectar mano derecha para seguimiento\n",
    "            if results.multi_handedness:\n",
    "                handedness = results.multi_handedness[hand_idx]\n",
    "                if handedness.classification[0].label == 'Left' and not x_normalized:\n",
    "                    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                    \n",
    "                    # Calcular coordenadas normalizadas\n",
    "                    x_normalized = int((wrist.x - 0.5) * 15)  # Rango -7.5 a 7.5\n",
    "                    #print(x_normalized)\n",
    "                    \n",
    "                    # Obtener coordenadas para dibujo\n",
    "                    right_wrist_pixel = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        wrist.x, wrist.y, frame.shape[1], frame.shape[0]\n",
    "                    )\n",
    "\n",
    "        # Enviar datos y dibujar si se detectó mano derecha\n",
    "        if x_normalized is not None:\n",
    "            if send_sock:  # Solo enviar si se provee socket\n",
    "                send_sock.sendto(\n",
    "                    str(x_normalized).encode(),\n",
    "                    (UDP_IP_PI, UDP_PORT_SERVO)  # Asegurar que estas constantes están definidas\n",
    "                )\n",
    "            if right_wrist_pixel:\n",
    "                cv2.circle(frame, right_wrist_pixel, 10, (0, 255, 0), -1)\n",
    "    \n",
    "    # Rellenar con ceros si no hay manos\n",
    "    while len(landmarks_data) < 21 * 3 * 2:\n",
    "        landmarks_data.append(0.0)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "\n",
    "        # Enviar estado de detección\n",
    "    current_time = time.time()\n",
    "    if hands_detected:\n",
    "        sock_hands_status.sendto(b\"HANDS_DETECTED\", (UDP_IP_PI, UDP_PORT_HANDS_STATUS))\n",
    "        last_hands_detected_time = current_time\n",
    "        hands_detected_status = True\n",
    "    else:\n",
    "        if current_time - last_hands_detected_time > 1:  # Esperar 2 segundos sin manos\n",
    "            sock_hands_status.sendto(b\"NO_HANDS\", (UDP_IP_PI, UDP_PORT_HANDS_STATUS))\n",
    "            hands_detected_status = False\n",
    "    \n",
    "    return landmarks_data, hands_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "62350ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3435cd",
   "metadata": {},
   "source": [
    "### CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "48579101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global data, labels\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "            data = loaded_data[\"features\"]\n",
    "            labels = loaded_data[\"labels\"]\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos: {e}\")\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a440d50",
   "metadata": {},
   "source": [
    "### RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bc72c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_exists():\n",
    "    return os.path.exists(model_file) and os.path.exists(scaler_file) and os.path.exists(encoder_file)\n",
    "\n",
    "def create_neural_network(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d1ad",
   "metadata": {},
   "source": [
    "### CARGA DEL MODELO ENTRENADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "454258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model():\n",
    "    global scaler, label_encoder\n",
    "    try:\n",
    "        model = load_model(model_file)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_file, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        set_message(\"Modelo cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {e}\")\n",
    "        set_message(\"Error al cargar el modelo\", 2)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651eed",
   "metadata": {},
   "source": [
    "### MODELO DE TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7e5be08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.5):\n",
    "    try:\n",
    "        # Preprocesar los landmarks\n",
    "        landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "        landmarks_scaled = scaler.transform(landmarks_array)\n",
    "        \n",
    "        # Realizar predicción\n",
    "        predictions = tflite_model.predict(landmarks_scaled)[0]\n",
    "        \n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        max_prob_idx = np.argmax(predictions)\n",
    "        confidence = predictions[max_prob_idx]\n",
    "        \n",
    "        if confidence >= threshold:\n",
    "            # Decodificar la etiqueta\n",
    "            predicted_label = label_encoder.inverse_transform([max_prob_idx])[0]\n",
    "            return predicted_label, confidence\n",
    "        else:\n",
    "            return \"Desconocido\", confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la predicción: {e}\")\n",
    "        return \"Error\", 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b02471",
   "metadata": {},
   "source": [
    "### CONVERSION A TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "32082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_file, model_tflite):\n",
    "    try:\n",
    "        if not os.path.exists(model_file):\n",
    "            raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "        \n",
    "        # Cargar el modelo entrenado\n",
    "        modelo = load_model(model_file)\n",
    "        \n",
    "        # Convertir a TensorFlow Lite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo convertido\n",
    "        with open(model_tflite, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al convertir el modelo a TFLite:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd15d98",
   "metadata": {},
   "source": [
    "### MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "89e8856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_menu():\n",
    "    print(\"\\n=== MENU PRINCIPAL ===\")\n",
    "    print(\"1. Listar señas cargadas\")\n",
    "    print(\"2. Evaluar en tiempo real\")\n",
    "    print(\"0. Salir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c02105",
   "metadata": {},
   "source": [
    "### LISTADO DE GESTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "81a424fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gestures():\n",
    "    # Asumiendo que 'labels' es la lista donde se guardan las señas\n",
    "    if not labels:\n",
    "        print(\"No hay señas guardadas.\")\n",
    "    else:\n",
    "        unique_gestures = list(set(labels))\n",
    "        print(\"\\n--- Señas Guardadas ---\")\n",
    "        for i, gesture in enumerate(unique_gestures, 1):\n",
    "            print(f\"{i}. {gesture}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43829529",
   "metadata": {},
   "source": [
    "### EVALUACION EN TIEMPO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_mode():\n",
    "    global model_tflite\n",
    "    # Inicializa el modelo TFLite si aún no se ha cargado\n",
    "    if os.path.exists(model_tflite):\n",
    "        tflite_model = TFLiteModel(model_tflite)\n",
    "    else:\n",
    "        print(\"El modelo TFLite no existe. Conviértelo primero.\")\n",
    "        return\n",
    "\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para evaluación en tiempo real.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    #NUEVO BARRITA DE PROGRESO\n",
    "    # Variables para el sistema de confirmación de señas\n",
    "    consecutive_frames = 0\n",
    "    last_prediction = \"\"\n",
    "    confirmation_threshold = 8  # Número de frames consecutivos necesarios\n",
    "    confirmed_gesture = \"\"\n",
    "    \n",
    "    # Para el enfoque alternativo de ventana deslizante\n",
    "    window_size = 5\n",
    "    prediction_window = []\n",
    "    \n",
    "    # Umbral de confianza para considerar una detección válida\n",
    "    confidence_threshold = 0.9\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame, send_sock)\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        #NUEVO BARRITA DE PROGRESO\n",
    "        if hands_detected:\n",
    "            prediction, confidence = predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=confidence_threshold)\n",
    "            \n",
    "            # Extraer valor escalar en caso de que 'confidence' sea un array\n",
    "            confidence_value = np.max(confidence) if isinstance(confidence, np.ndarray) else confidence\n",
    "            \n",
    "            # Color basado en la confianza\n",
    "            color = (0, 255, 0) if confidence_value > confidence_threshold else (0, 165, 255)\n",
    "            \n",
    "            # Mostrar la predicción actual\n",
    "            cv2.putText(frame, f\"Seña detectada: {prediction}\", (10, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "            cv2.putText(frame, f\"Confianza: {confidence_value:.2%}\", (10, 90), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Sistema de confirmación por frames consecutivos\n",
    "            if confidence_value > confidence_threshold and prediction != \"Desconocido\":\n",
    "                # Enfoque 1: Frames consecutivos\n",
    "                if prediction == last_prediction:\n",
    "                    consecutive_frames += 1\n",
    "                else:\n",
    "                    consecutive_frames = 1  # Reiniciar contador si cambia la predicción\n",
    "                \n",
    "                last_prediction = prediction\n",
    "                \n",
    "                # Actualizar la ventana deslizante para el enfoque alternativo\n",
    "                prediction_window.append(prediction)\n",
    "                if len(prediction_window) > window_size:\n",
    "                    prediction_window.pop(0)  # Mantener solo los últimos 'window_size' elementos\n",
    "                \n",
    "                # Mostrar barra de progreso para la confirmación\n",
    "                progress_width = int((consecutive_frames / confirmation_threshold) * 200)  # Ancho máximo de 200 píxeles\n",
    "                progress_width = min(progress_width, 200)  # Limitar al máximo\n",
    "                \n",
    "                # Dibujar barra de progreso\n",
    "                cv2.rectangle(frame, (10, 120), (210, 140), (100, 100, 100), -1)  # Fondo gris\n",
    "                cv2.rectangle(frame, (10, 120), (10 + progress_width, 140), (0, 255, 0), -1)  # Barra verde\n",
    "                cv2.putText(frame, f\"Confirmando: {consecutive_frames}/{confirmation_threshold}\", (10, 160), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "                \n",
    "                # Verificar si se ha alcanzado el umbral de confirmación\n",
    "                if consecutive_frames >= confirmation_threshold:\n",
    "                    if confirmed_gesture != prediction:  # Evitar repeticiones\n",
    "                        confirmed_gesture = prediction\n",
    "                        threading.Thread(target=speak_text, args=(prediction,), daemon=True).start()\n",
    "                        cv2.putText(frame, f\"¡CONFIRMADO!: {prediction}\", (frame_w//4, frame_h//2), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "                \n",
    "                # Enfoque alternativo: Seña más frecuente en la ventana\n",
    "                if len(prediction_window) == window_size:\n",
    "                    from collections import Counter\n",
    "                    most_common = Counter(prediction_window).most_common(1)[0]  # (elemento, frecuencia)\n",
    "                    most_common_gesture, frequency = most_common\n",
    "                    \n",
    "                    # Mostrar información sobre el enfoque alternativo\n",
    "                    cv2.putText(frame, f\"Más frecuente: {most_common_gesture} ({frequency}/{window_size})\", \n",
    "                                (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 1)\n",
    "                    \n",
    "                    # Este enfoque se puede habilitar si se prefiere sobre el de frames consecutivos\n",
    "                    # Si se quiere usar este enfoque en lugar del de frames consecutivos, descomentar:\n",
    "                    # if frequency >= 3 and confirmed_gesture != most_common_gesture:  # Mayoría en ventana de 5\n",
    "                    #     confirmed_gesture = most_common_gesture\n",
    "                    #     threading.Thread(target=speak_text, args=(most_common_gesture,), daemon=True).start()\n",
    "            else:\n",
    "                # Reiniciar contador si la confianza es baja\n",
    "                consecutive_frames = 0\n",
    "                cv2.putText(frame, \"Confianza insuficiente\", (10, 160), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "        #HASTA ACA LLEGA LA MODIFICACION DE LA BARRITA DE PROGRESO        \n",
    "\n",
    "        else:\n",
    "            # Reiniciar contador si no se detectan manos\n",
    "            consecutive_frames = 0\n",
    "            #HASTA ACA LLEGA LA MODIFICACION DE LA BARRITA DE PROGRESO \n",
    "\n",
    "            cv2.putText(frame, \"Acerca las manos a la cámara\", (frame_w//4, frame_h//2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"Presiona ESC para volver al menú\", (10, frame_h - 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.imshow(\"Evaluación en Tiempo Real\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b50ac",
   "metadata": {},
   "source": [
    "### FUNCION PRINCIPAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff93c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "\n",
    "    # Iniciar el servicio de reconocimiento de voz al iniciar el programa\n",
    "    print(\"Iniciando servicio de reconocimiento de voz...\")\n",
    "    speech_threads = start_speech_recognition()\n",
    "    print(\"Servicio de reconocimiento de voz iniciado correctamente.\")\n",
    "    \n",
    "    # Inicialización del sistema\n",
    "    is_trained = False\n",
    "    model = None\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Mostrar el menú en la consola\n",
    "    print_menu()\n",
    "    try:\n",
    "        # Bucle principal de selección en consola\n",
    "        while True:\n",
    "            opcion = input(\"\\nSelecciona una opción (Señas: 1, Evaluar: 2, Salir: 0): \").strip()\n",
    "                    \n",
    "            if opcion == '1':\n",
    "                list_gestures()  # Lista las señas cargadas\n",
    "\n",
    "            elif opcion == '2':\n",
    "                if is_trained:\n",
    "                    # Inicializar modo evaluación en tiempo real\n",
    "                    print(\"Modo de evaluación activado.\")\n",
    "                    run_evaluation_mode()\n",
    "                else:\n",
    "                    print(\"¡Entrena el modelo primero!\")\n",
    "                    \n",
    "            elif opcion == '0':\n",
    "                print(\"Deteniendo servicio de reconocimiento de voz...\")\n",
    "                stop_speech_recognition()\n",
    "                print(\"Saliendo del programa...\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Opción inválida, intenta nuevamente.\")\n",
    "            \n",
    "            # Mostrar nuevamente el menú luego de finalizar la opción seleccionada.\n",
    "            print_menu()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nCerrando programa...\")\n",
    "    finally:\n",
    "        # Enviar señal de salida\n",
    "        sock_heartbeat.sendto(b\"PROGRAM_EXIT\", (UDP_IP_PI, UDP_PORT_HEARTBEAT))\n",
    "        sock_heartbeat.close()\n",
    "        stop_speech_recognition()\n",
    "        print(\"Programa finalizado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbcb2f",
   "metadata": {},
   "source": [
    "# EJECUTAR PROGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "40955799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando servicio de reconocimiento de voz...\n",
      "Servicio de reconocimiento de voz iniciado...\n",
      "Servicio de reconocimiento de voz iniciado correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Listar señas cargadas\n",
      "2. Evaluar en tiempo real\n",
      "0. Salir\n",
      "No se detectó voz en el audio\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Listar señas cargadas\n",
      "2. Evaluar en tiempo real\n",
      "0. Salir\n",
      "Deteniendo servicio de reconocimiento de voz...\n",
      "Servicio de reconocimiento de voz detenido.\n",
      "Saliendo del programa...\n",
      "Servicio de reconocimiento de voz detenido.\n",
      "Programa finalizado correctamente.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
